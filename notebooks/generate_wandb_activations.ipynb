{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(\"Running on Colab\")\n",
        "\n",
        "    subprocess.run([\n",
        "        'git', 'clone', 'https://github.com/walligot/don_thesis.git'\n",
        "    ])\n",
        "\n",
        "    os.chdir('/content/don_thesis')\n",
        "    #%pip install git+https://github.com/mitchellostrow/DSA.git\n",
        "    %pip install git+https://github.com/Melina-Jingting/foundational_ssm.git\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    os.environ['HOME'] = '/content/drive/MyDrive/Thesis'\n",
        "    ROOT_PATH = '/content/don_thesis'\n",
        "    os.environ['ROOT_PATH'] = ROOT_PATH\n",
        "\n",
        "    wandb_config_path = '/content/drive/MyDrive/Colab/wandb.config.json'\n",
        "\n",
        "else:\n",
        "    #%pip install git+https://github.com/mitchellostrow/DSA.git\n",
        "\n",
        "    current_path = Path().resolve()\n",
        "    ROOT_PATH = None\n",
        "    for parent in [current_path] + list(current_path.parents):\n",
        "        if \"don_thesis\" in parent.name.lower():\n",
        "            ROOT_PATH = parent\n",
        "            os.environ['ROOT_PATH'] = str(ROOT_PATH)\n",
        "            break\n",
        "\n",
        "    if not ROOT_PATH:\n",
        "        raise FileNotFoundError(\"Directory with name 'don_thesis' not found.\")\n",
        "\n",
        "    print(\"Running locally or elsewhere\")\n",
        "    wandb_config_path = os.path.join(os.environ['ROOT_PATH'], 'config', 'wandb.config.json')\n",
        "\n",
        "# Set WANDB_CONFIG_PATH\n",
        "os.environ['WANDB_CONFIG_PATH'] = wandb_config_path\n",
        "\n",
        "# Load API key from JSON\n",
        "with open(wandb_config_path) as f:\n",
        "    config = json.load(f)\n",
        "    os.environ['WANDB_API_KEY'] = config['WANDB_API_KEY']\n",
        "\n",
        "%pip install pynwb\n",
        "%pip install equinox\n",
        "%pip uninstall -y temporaldata\n",
        "%pip install git+https://github.com/Melina-Jingting/temporaldata.git@melina-resample-irregular\n",
        "#%pip install equinox==0.12.2 jax==0.7.0 jaxlib==0.7.0\n",
        "\n",
        "print(f\"Root path: {ROOT_PATH}\")\n",
        "print(f\"WANDB config path: {wandb_config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY5mcZuGlH60",
        "outputId": "b53af278-bda2-49e6-cdd6-07ba654a4db7"
      },
      "id": "aY5mcZuGlH60",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Colab\n",
            "Collecting git+https://github.com/Melina-Jingting/foundational_ssm.git\n",
            "  Cloning https://github.com/Melina-Jingting/foundational_ssm.git to /tmp/pip-req-build-fmjzcbk3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Melina-Jingting/foundational_ssm.git /tmp/pip-req-build-fmjzcbk3\n",
            "  Resolved https://github.com/Melina-Jingting/foundational_ssm.git to commit 0946f85db8cb835434ac12f50e22c187e8824699\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from foundational_ssm==0.1.0) (0.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from foundational_ssm==0.1.0) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from foundational_ssm==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from foundational_ssm==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from foundational_ssm==0.1.0) (2.3.0)\n",
            "Collecting pytorch_brain (from foundational_ssm==0.1.0)\n",
            "  Downloading pytorch_brain-0.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from foundational_ssm==0.1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from foundational_ssm==0.1.0) (0.21.3)\n",
            "Requirement already satisfied: jax[cuda12] in /usr/local/lib/python3.12/dist-packages (from foundational_ssm==0.1.0) (0.5.3)\n",
            "Collecting equinox (from foundational_ssm==0.1.0)\n",
            "  Downloading equinox-0.13.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from foundational_ssm==0.1.0) (0.2.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from foundational_ssm==0.1.0) (5.9.5)\n",
            "Collecting jaxtyping>=0.2.20 (from equinox->foundational_ssm==0.1.0)\n",
            "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from equinox->foundational_ssm==0.1.0) (4.15.0)\n",
            "Collecting wadler-lindig>=0.1.0 (from equinox->foundational_ssm==0.1.0)\n",
            "  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]->foundational_ssm==0.1.0) (0.5.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]->foundational_ssm==0.1.0) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]->foundational_ssm==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12]->foundational_ssm==0.1.0) (1.16.1)\n",
            "Requirement already satisfied: jax-cuda12-plugin<=0.5.3,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]->foundational_ssm==0.1.0) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->foundational_ssm==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->foundational_ssm==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->foundational_ssm==0.1.0) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->foundational_ssm==0.1.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->foundational_ssm==0.1.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->foundational_ssm==0.1.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->foundational_ssm==0.1.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->foundational_ssm==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->foundational_ssm==0.1.0) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf->foundational_ssm==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax->foundational_ssm==0.1.0) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->foundational_ssm==0.1.0) (0.1.90)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->foundational_ssm==0.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->foundational_ssm==0.1.0) (2025.2)\n",
            "Collecting temporaldata>=0.1.3 (from pytorch_brain->foundational_ssm==0.1.0)\n",
            "  Downloading temporaldata-0.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting einops (from foundational_ssm==0.1.0)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hydra-core~=1.3.2 (from pytorch_brain->foundational_ssm==0.1.0)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting torchtyping~=0.1 (from pytorch_brain->foundational_ssm==0.1.0)\n",
            "  Downloading torchtyping-0.1.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting torchmetrics>=1.6.0 (from pytorch_brain->foundational_ssm==0.1.0)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_brain->foundational_ssm==0.1.0) (2.11.7)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from pytorch_brain->foundational_ssm==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->foundational_ssm==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->foundational_ssm==0.1.0) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->foundational_ssm==0.1.0) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->foundational_ssm==0.1.0) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb->foundational_ssm==0.1.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->foundational_ssm==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->foundational_ssm==0.1.0) (2.36.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->foundational_ssm==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->foundational_ssm==0.1.0) (4.0.12)\n",
            "Requirement already satisfied: jax-cuda12-pjrt==0.5.3 in /usr/local/lib/python3.12/dist-packages (from jax-cuda12-plugin<=0.5.3,>=0.5.3->jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]->foundational_ssm==0.1.0) (0.5.3)\n",
            "Collecting nvidia-cuda-nvcc-cu12>=12.6.85 (from jax-cuda12-plugin[with_cuda]<=0.5.3,>=0.5.3; extra == \"cuda12\"->jax[cuda12]->foundational_ssm==0.1.0)\n",
            "  Downloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->pytorch_brain->foundational_ssm==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->pytorch_brain->foundational_ssm==0.1.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic~=2.0->pytorch_brain->foundational_ssm==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->foundational_ssm==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb->foundational_ssm==0.1.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb->foundational_ssm==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb->foundational_ssm==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb->foundational_ssm==0.1.0) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->foundational_ssm==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: h5py>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from temporaldata>=0.1.3->pytorch_brain->foundational_ssm==0.1.0) (3.14.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics>=1.6.0->pytorch_brain->foundational_ssm==0.1.0)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting typeguard<3,>=2.11.1 (from torchtyping~=0.1->pytorch_brain->foundational_ssm==0.1.0)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->foundational_ssm==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->pytorch_brain->foundational_ssm==0.1.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->pytorch_brain->foundational_ssm==0.1.0) (2.19.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->foundational_ssm==0.1.0) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->pytorch_brain->foundational_ssm==0.1.0) (0.1.2)\n",
            "Downloading equinox-0.13.0-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.7/177.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_brain-0.1.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading temporaldata-0.1.3-py3-none-any.whl (30 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtyping-0.1.5-py3-none-any.whl (17 kB)\n",
            "Downloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\n",
            "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading nvidia_cuda_nvcc_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (40.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: foundational_ssm\n",
            "  Building wheel for foundational_ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for foundational_ssm: filename=foundational_ssm-0.1.0-py3-none-any.whl size=85092 sha256=a1a734b0759073988c195e0fd50e5179aac88f7989810de18babb01b7f244d55\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2mfzop9a/wheels/65/13/97/f863fc6d850769e2b26f56083ac2229869552beb0b41ce2e10\n",
            "Successfully built foundational_ssm\n",
            "Installing collected packages: wadler-lindig, typeguard, nvidia-cuda-nvcc-cu12, lightning-utilities, einops, jaxtyping, hydra-core, temporaldata, torchtyping, torchmetrics, equinox, pytorch_brain, foundational_ssm\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.4\n",
            "    Uninstalling typeguard-4.4.4:\n",
            "      Successfully uninstalled typeguard-4.4.4\n",
            "  Attempting uninstall: nvidia-cuda-nvcc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvcc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvcc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvcc-cu12-12.5.82\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.1\n",
            "    Uninstalling einops-0.8.1:\n",
            "      Successfully uninstalled einops-0.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed einops-0.6.1 equinox-0.13.0 foundational_ssm-0.1.0 hydra-core-1.3.2 jaxtyping-0.3.2 lightning-utilities-0.15.2 nvidia-cuda-nvcc-cu12-12.9.86 pytorch_brain-0.1.0 temporaldata-0.1.3 torchmetrics-1.8.2 torchtyping-0.1.5 typeguard-2.13.3 wadler-lindig-0.1.7\n",
            "Mounted at /content/drive\n",
            "Collecting pynwb\n",
            "  Using cached pynwb-3.1.2-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: h5py>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from pynwb) (3.14.0)\n",
            "Collecting hdmf<5,>=4.1.0 (from pynwb)\n",
            "  Using cached hdmf-4.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from pynwb) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from pynwb) (2.2.2)\n",
            "Requirement already satisfied: platformdirs>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from pynwb) (4.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pynwb) (2.9.0.post0)\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from hdmf<5,>=4.1.0->pynwb) (4.25.1)\n",
            "Collecting ruamel-yaml>=0.16 (from hdmf<5,>=4.1.0->pynwb)\n",
            "  Using cached ruamel.yaml-0.18.15-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->pynwb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->pynwb) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pynwb) (1.17.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->hdmf<5,>=4.1.0->pynwb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->hdmf<5,>=4.1.0->pynwb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->hdmf<5,>=4.1.0->pynwb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->hdmf<5,>=4.1.0->pynwb) (0.27.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel-yaml>=0.16->hdmf<5,>=4.1.0->pynwb)\n",
            "  Using cached ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=3.2.0->hdmf<5,>=4.1.0->pynwb) (4.15.0)\n",
            "Using cached pynwb-3.1.2-py3-none-any.whl (1.4 MB)\n",
            "Using cached hdmf-4.1.0-py3-none-any.whl (336 kB)\n",
            "Using cached ruamel.yaml-0.18.15-py3-none-any.whl (119 kB)\n",
            "Using cached ruamel.yaml.clib-0.2.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (754 kB)\n",
            "Installing collected packages: ruamel.yaml.clib, ruamel-yaml, hdmf, pynwb\n",
            "Successfully installed hdmf-4.1.0 pynwb-3.1.2 ruamel-yaml-0.18.15 ruamel.yaml.clib-0.2.12\n",
            "Requirement already satisfied: equinox in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
            "Requirement already satisfied: jax>=0.4.38 in /usr/local/lib/python3.12/dist-packages (from equinox) (0.5.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.20 in /usr/local/lib/python3.12/dist-packages (from equinox) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from equinox) (4.15.0)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from equinox) (0.1.7)\n",
            "Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.38->equinox) (0.5.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.38->equinox) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.38->equinox) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.38->equinox) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.38->equinox) (1.16.1)\n",
            "Found existing installation: temporaldata 0.1.3\n",
            "Uninstalling temporaldata-0.1.3:\n",
            "  Successfully uninstalled temporaldata-0.1.3\n",
            "Collecting git+https://github.com/Melina-Jingting/temporaldata.git@melina-resample-irregular\n",
            "  Cloning https://github.com/Melina-Jingting/temporaldata.git (to revision melina-resample-irregular) to /tmp/pip-req-build-sh9dkt8e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Melina-Jingting/temporaldata.git /tmp/pip-req-build-sh9dkt8e\n",
            "  Running command git checkout -b melina-resample-irregular --track origin/melina-resample-irregular\n",
            "  Switched to a new branch 'melina-resample-irregular'\n",
            "  Branch 'melina-resample-irregular' set up to track remote branch 'melina-resample-irregular' from 'origin'.\n",
            "  Resolved https://github.com/Melina-Jingting/temporaldata.git to commit 9b5a609ad718cdad5bb1bc72ec8e725d93f6a536\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=60.0.0 in /usr/local/lib/python3.12/dist-packages (from temporaldata==0.1.3) (75.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from temporaldata==0.1.3) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from temporaldata==0.1.3) (2.2.2)\n",
            "Requirement already satisfied: h5py>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from temporaldata==0.1.3) (3.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->temporaldata==0.1.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->temporaldata==0.1.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->temporaldata==0.1.3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->temporaldata==0.1.3) (1.17.0)\n",
            "Building wheels for collected packages: temporaldata\n",
            "  Building wheel for temporaldata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for temporaldata: filename=temporaldata-0.1.3-py3-none-any.whl size=31265 sha256=fe60629e8092af6c25444a88df0fbc3f800c39fb099d88b2e845ffcf2b50b1f8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ux5qdzg_/wheels/b4/8d/84/a8f5b8d2205b636eff877d7d6befcf2732fc522ba026251059\n",
            "Successfully built temporaldata\n",
            "Installing collected packages: temporaldata\n",
            "Successfully installed temporaldata-0.1.3\n",
            "Root path: /content/don_thesis\n",
            "WANDB config path: /content/drive/MyDrive/Colab/wandb.config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import foundational_ssm.models.s5 as s5  # adjust to actual package path\n",
        "\n",
        "def patched_call_with_activations(self, x, state, layer_keys):\n",
        "    activations = {}\n",
        "    def capture(k, v):\n",
        "        if layer_keys is None or k in layer_keys:\n",
        "            activations[k] = v\n",
        "\n",
        "    # normalisation\n",
        "    x, state = self.norm(x.T, state)\n",
        "    x = x.T\n",
        "\n",
        "    # SSM forward with activations\n",
        "    ssm_y, ssm_x = self.ssm.call_with_activations(x)\n",
        "    capture(\"ssm_x\", ssm_x)\n",
        "    capture(\"ssm_y\", ssm_y)\n",
        "\n",
        "    # GELU + GLU\n",
        "    post_gelu = jax.nn.gelu(ssm_y)\n",
        "    capture(\"ssm_post_gelu\", post_gelu)\n",
        "\n",
        "    post_glu = jax.vmap(self.glu)(post_gelu)\n",
        "    capture(\"ssm_post_glu\", post_glu)\n",
        "\n",
        "    # return the value to feed into the next block\n",
        "    return post_glu, state, activations\n",
        "\n",
        "# Patch the class\n",
        "s5.S5Block.call_with_activations = patched_call_with_activations"
      ],
      "metadata": {
        "id": "9a_8d6lr96v8"
      },
      "id": "9a_8d6lr96v8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fed3b20e",
      "metadata": {
        "id": "fed3b20e"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import equinox as eqx\n",
        "import os\n",
        "\n",
        "# Foundational SSM imports\n",
        "from omegaconf import OmegaConf\n",
        "import tempfile\n",
        "from foundational_ssm.models import SSMDownstreamDecoder, SSMFoundationalDecoder\n",
        "from foundational_ssm.utils import h5_to_dict\n",
        "from foundational_ssm.transform import smooth_spikes\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from typing import Any, BinaryIO\n",
        "\n",
        "\n",
        "#%load_ext autoreload\n",
        "#%autoreload 2\n",
        "\n",
        "def default_deserialise_filter_spec(f: BinaryIO, x: Any) -> Any:\n",
        "    \"\"\"Default filter specification for deserialising saved data.\n",
        "\n",
        "    **Arguments**\n",
        "\n",
        "    -   `f`: file-like object\n",
        "    -   `x`: The leaf for which the data needs to be loaded.\n",
        "\n",
        "    **Returns**\n",
        "\n",
        "    The new value for datatype `x`.\n",
        "\n",
        "    !!! info\n",
        "\n",
        "        This function can be extended to customise the deserialisation behaviour for\n",
        "        leaves.\n",
        "\n",
        "    !!! example\n",
        "\n",
        "        Skipping loading of jax.Array.\n",
        "\n",
        "        ```python\n",
        "        import jax.numpy as jnp\n",
        "        import equinox as eqx\n",
        "\n",
        "        tree = (jnp.array([4,5,6]), [1,2,3])\n",
        "        new_filter_spec = lambda f,x: (\n",
        "            x if isinstance(x, jax.Array) else eqx.default_deserialise_filter_spec(f, x)\n",
        "        )\n",
        "        new_tree = eqx.tree_deserialise_leaves(\"some_filename.eqx\", tree, filter_spec=new_filter_spec)\n",
        "        ```\n",
        "    \"\"\"  # noqa: E501\n",
        "    try:\n",
        "        if isinstance(x, (jax.Array, jax.ShapeDtypeStruct)):\n",
        "            return jnp.load(f)\n",
        "        elif isinstance(x, np.ndarray):\n",
        "            # Important to use `np` here to avoid promoting NumPy arrays to JAX.\n",
        "            return np.load(f)\n",
        "        elif eqx.is_array_like(x):\n",
        "            # np.generic gets deserialised directly as an array, so convert back to a scalar\n",
        "            # type here.\n",
        "            # See also https://github.com/google/jax/issues/17858\n",
        "            out = np.load(f)\n",
        "            if isinstance(x, jax.dtypes.bfloat16):\n",
        "                out = out.view(jax.dtypes.bfloat16)\n",
        "            if np.size(out) == 1:\n",
        "                return type(x)(out.item())\n",
        "        else:\n",
        "            return x\n",
        "    except:\n",
        "        print(\"Failed to load data for leaf with shape/ value:\", x.shape if hasattr(x, 'shape') else x)\n",
        "        return x\n",
        "\n",
        "def load_model_and_state_from_checkpoint_wandb(artifact_full_name, model_cls=SSMDownstreamDecoder, model_cfg=None):\n",
        "    \"\"\"Load model, optimizer state, epoch, and step from a checkpoint file.\"\"\"\n",
        "    api = wandb.Api()\n",
        "    try:\n",
        "        artifact = api.artifact(artifact_full_name, type=\"checkpoint\")\n",
        "    except Exception as e:\n",
        "        raise FileNotFoundError(f\"Could not find checkpoint artifact: {artifact_full_name}\")\n",
        "\n",
        "    if model_cfg is None:\n",
        "        run = artifact.logged_by()\n",
        "        run_cfg = OmegaConf.create(run.config)\n",
        "        print(run_cfg)\n",
        "        model_cfg = OmegaConf.create(run_cfg.model)\n",
        "\n",
        "    model_template, state_template = eqx.nn.make_with_state(model_cls)(\n",
        "        **model_cfg\n",
        "    )\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        artifact.download(temp_dir)\n",
        "        model = eqx.tree_deserialise_leaves(os.path.join(temp_dir, \"model.ckpt\"), model_template, default_deserialise_filter_spec)\n",
        "        state = eqx.tree_deserialise_leaves(os.path.join(temp_dir, \"state.ckpt\"), state_template, default_deserialise_filter_spec)\n",
        "\n",
        "    meta = artifact.metadata\n",
        "    return model, state, meta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97ab55e",
      "metadata": {
        "id": "d97ab55e"
      },
      "source": [
        "# Downstream Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b15f7799",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "b15f7799",
        "outputId": "30e596f2-14b5-4740-878e-40011688a9d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_model_and_state_from_checkpoint_wandb' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2690442913.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# epoch 0 now stores a fresh model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0martifact_full_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"melinajingting-ucl/foundational_ssm_rtt/l{layer}_{pretrain_mode}_{train_mode}_checkpoint:{alias}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel_2_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_2_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_and_state_from_checkpoint_wandb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_full_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_model_and_state_from_checkpoint_wandb' is not defined"
          ]
        }
      ],
      "source": [
        "layer = \"2\"\n",
        "pretrain_mode = \"scratch\"\n",
        "train_mode = \"all\"\n",
        "alias = \"best\" # can be latest/best/ epoch_{any value in range(0,1000,100)}\n",
        "# epoch 0 now stores a fresh model.\n",
        "artifact_full_name = f\"melinajingting-ucl/foundational_ssm_rtt/l{layer}_{pretrain_mode}_{train_mode}_checkpoint:{alias}\"\n",
        "model_2_block, state_2_block, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name)\n",
        "\n",
        "layer = \"4\"\n",
        "artifact_full_name = f\"melinajingting-ucl/foundational_ssm_rtt/l{layer}_{pretrain_mode}_{train_mode}_checkpoint:{alias}\"\n",
        "model_4_block, state_4_block, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50c1b41",
      "metadata": {
        "id": "a50c1b41"
      },
      "source": [
        "## Calling with activations (Downstream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c66262",
      "metadata": {
        "id": "42c66262"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "# Download mc_rtt_trialized from https://huggingface.co/datasets/MelinaLaimon/nlb_processed/tree/main\n",
        "# Edit dataset_dir to your directory\n",
        "dataset_dir = \"/content/drive/MyDrive/Thesis/data/\"\n",
        "dataset_path = os.path.join(dataset_dir, \"mc_rtt_trialized.h5\")\n",
        "data = h5_to_dict(dataset_path)\n",
        "data[\"neural_input_raw\"] = copy.deepcopy(data[\"neural_input\"])\n",
        "data[\"neural_input\"] = smooth_spikes(data[\"neural_input\"], kern_sd_ms=20, bin_size_ms=5, time_axis=1)\n",
        "input = data[\"neural_input\"]\n",
        "target_vel = data[\"behavior_input\"]\n",
        "data[\"targets\"] = copy.deepcopy(data[\"behavior_input\"])\n",
        "\n",
        "# Specify the layers you want to generate the activations of.\n",
        "# [\"post_encoder\", \"ssm_pre_activation\", \"ssm_post_activation\"]\n",
        "layer_keys = [\"ssm_x\", \"ssm_y\", \"ssm_post_glu\"]\n",
        "inf_model = eqx.nn.inference_mode(model_2_block) # Switches off dropout\n",
        "pred_vel, _, activations_2_block = jax.vmap(inf_model.call_with_activations, axis_name=\"batch\", in_axes=(0, None, None))(input, state_2_block, layer_keys)\n",
        "activations_2_block['neural_input_raw'] = data['neural_input_raw']\n",
        "activations_2_block['neural_input'] = data['neural_input']\n",
        "activations_2_block['targets'] = data['targets']\n",
        "activations_2_block_dict = {}\n",
        "activations_2_block_dict['mc_rtt'] = activations_2_block\n",
        "\n",
        "inf_model = eqx.nn.inference_mode(model_4_block) # Switches off dropout\n",
        "pred_vel, _, activations_4_block = jax.vmap(inf_model.call_with_activations, axis_name=\"batch\", in_axes=(0, None, None))(input, state_4_block, layer_keys)\n",
        "activations_4_block['neural_input_raw'] = data['neural_input_raw']\n",
        "activations_4_block['neural_input'] = data['neural_input']\n",
        "activations_4_block['targets'] = data['targets']\n",
        "activations_4_block_dict = {}\n",
        "activations_4_block_dict['mc_rtt'] = activations_4_block"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savez(dataset_dir + \"activations_rtt_2block_20250831_2.npz\", **activations_2_block_dict)\n",
        "np.savez(dataset_dir + \"activations_rtt_4block_20250831_2.npz\", **activations_4_block_dict)"
      ],
      "metadata": {
        "id": "e1m-qG8qnBZ1"
      },
      "id": "e1m-qG8qnBZ1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activations_2_block_dict['mc_rtt'].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBxADI-7aJrZ",
        "outputId": "b8fec0a7-d562-4d65-d890-0c40dd387409"
      },
      "id": "sBxADI-7aJrZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['ssm_post_gelu_0', 'ssm_post_gelu_1', 'ssm_x_0', 'ssm_x_1', 'ssm_y_0', 'ssm_y_1', 'neural_input_raw', 'neural_input', 'targets'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "def add_prev_timestep(data):\n",
        "    \"\"\"\n",
        "    Append previous timestep features to each timestep within a trial.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray, shape (n_trials, n_timesteps, n_dims)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    out : np.ndarray, shape (n_trials, n_timesteps, 2*n_dims)\n",
        "          For t=0, previous timestep is all zeros.\n",
        "    \"\"\"\n",
        "    N, T, D = data.shape\n",
        "\n",
        "    # Shift along time axis within each trial\n",
        "    prev = np.zeros_like(data)\n",
        "    prev[:, 1:, :] = data[:, :-1, :]\n",
        "\n",
        "    # Concatenate current and previous features\n",
        "    out = np.concatenate([prev, data], axis=2)\n",
        "    return out\n",
        "\n",
        "def simple_linear_decoder_by_trial(hidden_states, behaviour, test_size=0.2,\n",
        "                                   alphas=[1e-3, 1e-2, 1e-1, 1], top_n=10):\n",
        "    \"\"\"\n",
        "    Trains a linear decoder from hidden states to behaviour, splitting by trial.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    hidden_states : array, shape (n_trials, n_timesteps, n_features)\n",
        "    behaviour     : array, shape (n_trials, n_timesteps, n_outputs)\n",
        "                    or (n_trials, n_timesteps) for single output\n",
        "    test_size     : float, fraction of trials to hold out\n",
        "    alphas        : list, RidgeCV regularisation strengths\n",
        "    top_n         : int, number of top features to return\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    R2 : float, held-out R^2 score (mean over outputs if multi-output)\n",
        "    weights : np.ndarray, decoder coefficients (n_outputs, n_features)\n",
        "              or (n_features,) if single output\n",
        "    top_indices : np.ndarray, indices of top N contributing features\n",
        "    \"\"\"\n",
        "    hs = np.asarray(hidden_states)\n",
        "    beh = np.asarray(behaviour)\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    # Handle behaviour shape\n",
        "    if beh.ndim == 2:  # (N, T) single output\n",
        "        B = 1\n",
        "        beh = beh[:, :, None]  # add output dim\n",
        "    elif beh.ndim == 3:  # (N, T, B)\n",
        "        B = beh.shape[2]\n",
        "    else:\n",
        "        raise ValueError(\"behaviour must be (N, T) or (N, T, B)\")\n",
        "\n",
        "    # Split by trial\n",
        "    trial_indices = np.arange(N)\n",
        "    train_trials, test_trials = train_test_split(\n",
        "        trial_indices, test_size=test_size, random_state=0\n",
        "    )\n",
        "\n",
        "    # Flatten over timesteps within each split\n",
        "    Xtr = hs[train_trials].reshape(-1, H)\n",
        "    Xte = hs[test_trials].reshape(-1, H)\n",
        "    ytr = beh[train_trials].reshape(-1, B)\n",
        "    yte = beh[test_trials].reshape(-1, B)\n",
        "\n",
        "    # Fit ridge regression\n",
        "    decoder = RidgeCV(alphas=alphas).fit(Xtr, ytr)\n",
        "    R2 = decoder.score(Xte, yte)\n",
        "\n",
        "    # Feature importance\n",
        "    importance = np.abs(decoder.coef_).sum(axis=0)  # sum over outputs if multi-output\n",
        "    top_indices = np.argsort(importance)[::-1][:top_n]\n",
        "\n",
        "    # Return shape for weights: squeeze if single output\n",
        "    weights = decoder.coef_.squeeze() if B == 1 else decoder.coef_\n",
        "\n",
        "    return R2, weights, top_indices"
      ],
      "metadata": {
        "id": "fIreYoEysdNP"
      },
      "id": "fIreYoEysdNP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "encode_t = jax.vmap(model_2_block.decoder, in_axes=0)  # over time\n",
        "encode_bt = jax.vmap(encode_t, in_axes=0)\n",
        "decoded = encode_bt(activations_2_block['ssm_post_glu_1'])\n",
        "decoded_flat = decoded.reshape(-1, decoded.shape[-1])\n",
        "input_flat = data['behavior_input'].reshape(-1, data['behavior_input'].shape[-1])\n",
        "\n",
        "decoded.reshape(-1, decoded.shape[-1])\n",
        "data['behavior_input'].reshape(-1, data['behavior_input'].shape[-1])\n",
        "\n",
        "r2_score(decoded_flat, input_flat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n_ZXSd4vz-L",
        "outputId": "919a19e1-2095-4354-9a60-c86275dc00e2"
      },
      "id": "-n_ZXSd4vz-L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-137.2387528681632"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(data[\"neural_input\"], data['behavior_input'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "8g5G89VDse3E",
        "outputId": "fd14d817-88c6-47f4-ccd9-b405a990082d"
      },
      "id": "8g5G89VDse3E",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with dim 3. None expected <= 2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1270799549.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"neural_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'behavior_input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m     _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[0;32m-> 1257\u001b[0;31m         _check_reg_targets_with_floating_dtype(\n\u001b[0m\u001b[1;32m   1258\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets_with_floating_dtype\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mdtype_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_matching_floating_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m             )\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0ae424",
      "metadata": {
        "id": "2b0ae424"
      },
      "source": [
        "## Example: Plotting Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ada5ca1",
      "metadata": {
        "id": "4ada5ca1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from foundational_ssm.plotting import aggregate_bin_label_results, plot_pred_vs_targets_by_angle_bin\n",
        "\n",
        "\n",
        "# Download mc_rtt_trialized from https://huggingface.co/datasets/MelinaLaimon/nlb_processed/tree/main\n",
        "# Edit dataset_dir to your directory\n",
        "dataset_dir = \"../../data/foundational_ssm/processed/nlb\"\n",
        "trial_info = pd.read_csv(os.path.join(dataset_dir, \"mc_rtt_trialized.csv\"))\n",
        "dataset_path = os.path.join(dataset_dir, \"mc_rtt_trialized.h5\")\n",
        "data = h5_to_dict(dataset_path)\n",
        "data[\"neural_input\"] = smooth_spikes(data[\"neural_input\"], kern_sd_ms=20, bin_size_ms=5, time_axis=1)\n",
        "input = data[\"neural_input\"]\n",
        "target_vel = data[\"behavior_input\"]\n",
        "\n",
        "# Specify the layers you want to generate the activations of.\n",
        "# [\"post_encoder\", \"ssm_pre_activation\", \"ssm_post_activation\"]\n",
        "layer_keys = [\"ssm_pre_activation\"]\n",
        "inf_model = eqx.nn.inference_mode(model) # Switches off dropout\n",
        "pred_vel, _, activations = jax.vmap(inf_model.call_with_activations, axis_name=\"batch\", in_axes=(0, None, None))(input, state, layer_keys)\n",
        "\n",
        "results_df = aggregate_bin_label_results(trial_info, target_vel, pred_vel)\n",
        "fig = plot_pred_vs_targets_by_angle_bin(results_df)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "221e3254",
      "metadata": {
        "id": "221e3254"
      },
      "source": [
        "# Foundational Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb84781f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb84781f",
        "outputId": "c38c01fc-5c6e-4ead-a50a-49caa21293fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavekk\u001b[0m (\u001b[33mdavekk-ucl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.03, 'output_dim': 2, 'ssm_io_dim': 256, 'context_dim': 0, 'ssm_num_layers': 2, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'masking'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_pretrain', 'resume_run_id': None}, 'rng_seed': 42, 'training': {'epochs': 501, 'log_val_every': 50, 'checkpoint_every': 1}, 'model_cfg': 'configs/model/l2_no_context.yaml', 'optimizer': {'lr': 0.002, 'mode': 'all', 'weight_decay': 0.01}, 'val_loader': {'sampler': 'TrialSampler', 'dataloader_args': {'batch_size': 512, 'num_workers': 4, 'persistent_workers': True}}, 'dataset_args': {'config': 'configs/dataset/reaching.yaml'}, 'train_loader': {'sampler': 'RandomVariableWindowSampler', 'sampler_args': {'drop_short': True, 'max_window_length': 5, 'min_window_length': 1}, 'dataloader_args': {'batch_size': 512, 'num_workers': 20, 'persistent_workers': True}}, 'sampling_rate': 200, 'prepend_history': 0.3}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model': {'dt_max': 0.01, 'dt_min': 0.001, 'ssm_dim': 128, 'rng_seed': 42, 'dropout_p': 0.01, 'output_dim': 2, 'ssm_io_dim': 256, 'context_dim': 0, 'ssm_num_layers': 4, 'ssm_init_diag_blocks': 4}, 'wandb': {'tags': ['neural', 'behavior', 'masking'], 'entity': 'melinajingting-ucl', 'project': 'foundational_ssm_pretrain', 'resume_run_id': None}, 'rng_seed': 42, 'training': {'epochs': 501, 'log_val_every': 50, 'checkpoint_every': 1}, 'model_cfg': 'configs/model/l4.yaml', 'optimizer': {'lr': 0.001, 'mode': 'all', 'weight_decay': 0.01}, 'val_loader': {'sampler': 'TrialSampler', 'dataloader_args': {'batch_size': 512, 'num_workers': 4, 'persistent_workers': True}}, 'dataset_args': {'config': 'configs/dataset/reaching.yaml'}, 'train_loader': {'sampler': 'RandomVariableWindowSampler', 'sampler_args': {'drop_short': True, 'max_window_length': 5, 'min_window_length': 1}, 'dataloader_args': {'batch_size': 512, 'num_workers': 20, 'persistent_workers': True}}, 'sampling_rate': 200, 'prepend_history': 0.3}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = \"l2\"\n",
        "#dataset = \"reaching_normalized\"\n",
        "dataset = \"reaching\"\n",
        "#alias = \"best\"\n",
        "alias = \"latest\"\n",
        "\n",
        "artifact_full_name = f\"melinajingting-ucl/foundational_ssm_pretrain/{model}_{dataset}_checkpoint:{alias}\"\n",
        "foundational_model_2block, foundational_state_2block, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name, model_cls=SSMFoundationalDecoder)\n",
        "\n",
        "model = \"l4\"\n",
        "artifact_full_name = f\"melinajingting-ucl/foundational_ssm_pretrain/{model}_{dataset}_checkpoint:{alias}\"\n",
        "foundational_model_4block, foundational_state_4block, meta = load_model_and_state_from_checkpoint_wandb(artifact_full_name, model_cls=SSMFoundationalDecoder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blk = foundational_model_2block.ssm_blocks[0]\n",
        "print(blk.call_with_activations.__func__ is patched_call_with_activations)  # True\n",
        "print(blk.call_with_activations.__name__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQk4VVS--aWx",
        "outputId": "5b3f2f95-d48f-4343-c6d7-4bbb0d7a60c6"
      },
      "id": "WQk4VVS--aWx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "patched_call_with_activations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3e7019",
      "metadata": {
        "id": "2a3e7019"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from foundational_ssm.constants import DATA_ROOT, MAX_NEURAL_UNITS, DATASET_GROUP_INFO\n",
        "from foundational_ssm.dataset import TorchBrainDataset\n",
        "from foundational_ssm.transform import transform_brainsets_regular_time_series_smoothed, parse_session_id\n",
        "from foundational_ssm.collate import pad_collate\n",
        "import foundational_ssm.samplers as samplers\n",
        "import numpy as np\n",
        "\n",
        "from typing import Dict, Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "\n",
        "from foundational_ssm.constants import (\n",
        "    DATASET_GROUP_TO_IDX,\n",
        "    MAX_NEURAL_UNITS,\n",
        "    MAX_BEHAVIOR_DIM,\n",
        "    DATASET_IDX_TO_STD\n",
        ")\n",
        "from foundational_ssm.spikes import bin_spikes, smooth_spikes\n",
        "\n",
        "def parse_session_id(session_id: str) -> Tuple[str, str, str]:\n",
        "    patterns = {\n",
        "        \"churchland_shenoy_neural_2012\": re.compile(r\"([^/]+)/([^_]+)_[0-9]+_(.+)\"),\n",
        "        \"flint_slutzky_accurate_2012\": re.compile(r\"([^/]+)/monkey_([^_]+)_e1_(.+)\"),\n",
        "        \"odoherty_sabes_nonhuman_2017\": re.compile(r\"([^/]+)/([^_]+)_[0-9]{8}_[0-9]+\"),\n",
        "        \"pei_pandarinath_nlb_2021\": re.compile(r\"([^/]+)/([^_]+)_(.+)\"),\n",
        "        \"perich_miller_population_2018\": re.compile(r\"([^/]+)/([^_]+)_[0-9]+_(.+)\"),\n",
        "    }\n",
        "\n",
        "    dataset = session_id.split('/')[0]\n",
        "    if dataset not in patterns:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset}\")\n",
        "\n",
        "    match = patterns[dataset].match(session_id)\n",
        "    if not match:\n",
        "        raise ValueError(f\"Could not parse session_id: {session_id!r}\")\n",
        "\n",
        "    if dataset == \"odoherty_sabes_nonhuman_2017\":\n",
        "        # Always assign task as 'random_target_reaching'\n",
        "        _, subject = match.groups()\n",
        "        return dataset, subject, \"random_target_reaching\"\n",
        "    elif dataset == \"flint_slutzky_accurate_2012\":\n",
        "        # task is always 'center_out_reaching'\n",
        "        _, subject, _ = match.groups()\n",
        "        return dataset, subject, \"center_out_reaching\"\n",
        "    else:\n",
        "        return match.groups()\n",
        "\n",
        "def _ensure_dim(arr: np.ndarray, target_dim: int, *, axis: int = 1) -> np.ndarray:\n",
        "    \"\"\"Crop or zero-pad *arr* along *axis* to match *target_dim*.\n",
        "\n",
        "    This is a thin wrapper around :pymod:`numpy` slicing and :func:`numpy.pad` that\n",
        "    avoids several conditional blocks in the main routine.\n",
        "    \"\"\"\n",
        "    current_dim = arr.shape[axis]\n",
        "    if current_dim == target_dim:\n",
        "        return arr  # nothing to do\n",
        "    if current_dim > target_dim:\n",
        "        # Crop\n",
        "        slicer = [slice(None)] * arr.ndim\n",
        "        slicer[axis] = slice(None, target_dim)\n",
        "        return arr[tuple(slicer)]\n",
        "    # Pad (current_dim < target_dim)\n",
        "    pad_width = [(0, 0)] * arr.ndim\n",
        "    pad_width[axis] = (0, target_dim - current_dim)\n",
        "    return np.pad(arr, pad_width, mode=\"constant\")\n",
        "\n",
        "def transform_brainsets_regular_time_series_raw(\n",
        "    data: Any,\n",
        "    *,\n",
        "    max_neural_units: int = MAX_NEURAL_UNITS,\n",
        "    sampling_rate: int = 200,\n",
        ") -> Dict[str, torch.Tensor | str]:\n",
        "    \"\"\"\n",
        "    Like `transform_brainsets_regular_time_series_smoothed` but WITHOUT smoothing.\n",
        "    Produces raw binned spike counts at `sampling_rate`, aligned with behaviour.\n",
        "    \"\"\"\n",
        "    # ----------------------------\n",
        "    # 1) Raw binned spikes (no smoothing)\n",
        "    # ----------------------------\n",
        "    # Always bin from spike indices to avoid using any pre-smoothed fields.\n",
        "    binned_spikes, _ = data.spikes.get_regular_time_series_array(\n",
        "        sampling_rate=sampling_rate,\n",
        "        raw_array_name=\"unit_index\",\n",
        "        is_index=True,            # yields raw spike counts per bin\n",
        "    )  # shape: (timesteps, units)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 2) Behaviour (cursor/hand velocity)\n",
        "    # ----------------------------\n",
        "    if hasattr(data, \"vel_regular\"):\n",
        "        behavior_input = data.vel_regular.data\n",
        "    else:\n",
        "        if data.session.id.startswith(\"pei_pandarinath_nlb_2021\"):\n",
        "            behavior_input, _ = data.hand.get_regular_time_series_array(\n",
        "                sampling_rate=sampling_rate,\n",
        "                raw_array_name=\"vel\",\n",
        "            )\n",
        "        else:\n",
        "            behavior_input, _ = data.cursor.get_regular_time_series_array(\n",
        "                sampling_rate=sampling_rate,\n",
        "                raw_array_name=\"vel\",\n",
        "            )\n",
        "\n",
        "    # ----------------------------\n",
        "    # 3) Drop timesteps with invalid behaviour\n",
        "    # ----------------------------\n",
        "    valid_mask = ~(np.isinf(behavior_input).any(axis=1) | np.isnan(behavior_input).any(axis=1))\n",
        "    if not valid_mask.all():\n",
        "        behavior_input = behavior_input[valid_mask]\n",
        "        binned_spikes = binned_spikes[valid_mask]\n",
        "\n",
        "    # ----------------------------\n",
        "    # 4) Group/index info, padding & normalisation\n",
        "    # ----------------------------\n",
        "    dataset, subject, task = parse_session_id(data.session.id)\n",
        "    group_tuple = (dataset, subject, task)\n",
        "    try:\n",
        "        group_idx = DATASET_GROUP_TO_IDX[group_tuple]\n",
        "    except Exception:\n",
        "        group_idx = 9\n",
        "\n",
        "    match = re.findall(r\"\\d+\", data.session.id.split(\"/\")[1])\n",
        "    session_date = int(\"\".join(match)) if len(match) > 0 else 0\n",
        "\n",
        "    # pad/crop neural to max_neural_units\n",
        "    neural_input = _ensure_dim(binned_spikes, max_neural_units, axis=1)\n",
        "\n",
        "    # normalise behaviour by dataset std, then pad/crop to MAX_BEHAVIOR_DIM\n",
        "    behavior_input = behavior_input / DATASET_IDX_TO_STD[group_idx]\n",
        "    behavior_input = _ensure_dim(behavior_input, MAX_BEHAVIOR_DIM, axis=1)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 5) Pack tensors\n",
        "    # ----------------------------\n",
        "    return {\n",
        "        \"neural_input\": torch.as_tensor(neural_input, dtype=torch.float32),\n",
        "        \"behavior_input\": torch.as_tensor(behavior_input, dtype=torch.float32),\n",
        "        \"dataset_group_idx\": torch.as_tensor(group_idx, dtype=torch.int32),\n",
        "        \"session_date\": torch.as_tensor(session_date, dtype=torch.int32),\n",
        "    }\n",
        "\n",
        "def get_brainset_data_loader_raw(\n",
        "    dataset_args,\n",
        "    dataloader_args,\n",
        "    sampler,\n",
        "    split = None,\n",
        "    sampler_args = {},\n",
        "    sampling_rate = 200,\n",
        "    prepend_history = 0,\n",
        "    data_root = DATA_ROOT,\n",
        "):\n",
        "    dataset = TorchBrainDataset(\n",
        "        root=data_root,                # root directory where .h5 files are found\n",
        "        **dataset_args,\n",
        "        split=split\n",
        "    )\n",
        "\n",
        "    sampling_intervals = dataset.get_sampling_intervals()\n",
        "    sampler_cls = getattr(samplers, sampler)\n",
        "    sampler = sampler_cls(\n",
        "        sampling_intervals=sampling_intervals,\n",
        "        **(sampler_args or {}),\n",
        "        prepend_history=prepend_history\n",
        "    )\n",
        "    max_neural_units = int(np.max( [DATASET_GROUP_INFO[parse_session_id(k)][\"max_num_units\"] for k in sampling_intervals.keys()]))\n",
        "    #dataset.transform = partial(transform_brainsets_regular_time_series_smoothed, sampling_rate=sampling_rate, max_neural_units=max_neural_units)\n",
        "    dataset.transform = partial(transform_brainsets_regular_time_series_raw, sampling_rate=sampling_rate, max_neural_units=max_neural_units)\n",
        "    total_window_length = sampler_args.get('window_length', sampler_args.get('max_window_length', 1)) + prepend_history  # Default to 1 if not provided\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,      # dataset\n",
        "        sampler=sampler,      # sampler\n",
        "        collate_fn=partial(pad_collate, fixed_seq_len=int(total_window_length*sampling_rate)),         # the collator\n",
        "        pin_memory=True,\n",
        "        **dataloader_args\n",
        "    )\n",
        "    return dataset, loader, max_neural_units"
      ],
      "metadata": {
        "id": "fGsM9NuFvyjI"
      },
      "id": "fGsM9NuFvyjI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing as mp\n",
        "\n",
        "# Foundational SSM core imports\n",
        "from foundational_ssm.loaders import get_brainset_data_loader, get_brainset_train_val_loaders\n",
        "#from foundational_ssm.constants import DATA_ROOT\n",
        "from foundational_ssm.samplers import TrialSampler\n",
        "import os\n",
        "\n",
        "data_root = '/content/drive/MyDrive/Thesis/data/foundational_ssm/processed'\n",
        "\n",
        "dataset, loader, max_neural_input = get_brainset_data_loader(\n",
        "    dataset_args = {\n",
        "        'keep_files_open': False,\n",
        "        'lazy': True,\n",
        "        'config': '/content/drive/MyDrive/Thesis/data/reaching.yaml'\n",
        "    },\n",
        "    dataloader_args={\n",
        "        'batch_size': 1500,\n",
        "        'num_workers': 0,\n",
        "        'persistent_workers': False,\n",
        "    },\n",
        "    sampler = 'TrialSampler',\n",
        "    sampler_args = {\n",
        "        'max_window_length': 5.0\n",
        "    },\n",
        "    data_root = data_root,\n",
        "    prepend_history = 0.3,\n",
        "    sampling_rate = 200,\n",
        "    split = 'val_trial' #train_trial overlaps with training data. use val_trial, test_trial if you want to observe what happens outside\n",
        " )\n",
        "\n",
        "dataset_raw, loader_raw, max_neural_input_raw = get_brainset_data_loader_raw(\n",
        "    dataset_args = {\n",
        "        'keep_files_open': False,\n",
        "        'lazy': True,\n",
        "        'config': '/content/drive/MyDrive/Thesis/data/reaching.yaml'\n",
        "    },\n",
        "    dataloader_args={\n",
        "        'batch_size': 1500,\n",
        "        'num_workers': 0,\n",
        "        'persistent_workers': False,\n",
        "    },\n",
        "    sampler = 'TrialSampler',\n",
        "    sampler_args = {\n",
        "        'max_window_length': 5.0\n",
        "    },\n",
        "    data_root = data_root,\n",
        "    prepend_history = 0.3,\n",
        "    sampling_rate = 200,\n",
        "    split = 'val_trial' #train_trial overlaps with training data. use val_trial, test_trial if you want to observe what happens outside\n",
        " )"
      ],
      "metadata": {
        "id": "BvrUvDuvF6eF"
      },
      "id": "BvrUvDuvF6eF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from foundational_ssm.constants import DATASET_IDX_TO_GROUP_SHORT\n",
        "from tqdm import tqdm\n",
        "input_by_dataset = {}\n",
        "skip_timesteps=56\n",
        "\n",
        "for batch_idx, batch in enumerate(tqdm(loader_raw, desc=\"Batches\")):\n",
        "        batch = {k: jax.device_put(np.array(v)) for k, v in batch.items()}\n",
        "        dataset_group_idxs = batch[\"dataset_group_idx\"]\n",
        "        inputs = batch[\"neural_input\"]\n",
        "        for i in range(inputs.shape[0]):\n",
        "            ds_id = int(dataset_group_idxs[i])\n",
        "\n",
        "            if ds_id not in input_by_dataset:\n",
        "                input_by_dataset[ds_id] = {\"neural_input_raw\": []}\n",
        "            inputs_sliced = inputs[i][skip_timesteps:]\n",
        "\n",
        "            input_by_dataset[ds_id][\"neural_input_raw\"].append(inputs_sliced)\n",
        "\n",
        "for ds_id in input_by_dataset:\n",
        "        input_by_dataset[ds_id] = {\n",
        "            k: jnp.stack(v, axis=0) for k, v in input_by_dataset[ds_id].items()\n",
        "        }\n",
        "\n",
        "input_by_dataset = {str(k): np.array(v) for k, v in input_by_dataset.items()}\n",
        "\n",
        "np.savez(\"/content/drive/MyDrive/Thesis/data/f_input_raw_20250910_1.npz\", **input_by_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P62RMHKqxEnl",
        "outputId": "2de197d2-3134-4c22-b82e-7cc15cfce331"
      },
      "id": "P62RMHKqxEnl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBatches:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Batches: 100%|██████████| 1/1 [00:15<00:00, 15.47s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.count_nonzero(input_by_dataset['pm_c_co']['neural_input_raw']))\n",
        "print(input_by_dataset['pm_c_co']['neural_input_raw'].size)\n",
        "#input_by_dataset.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seyiKrLMkVYh",
        "outputId": "981157a5-5331-489d-fd89-797937df4669"
      },
      "id": "seyiKrLMkVYh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29050\n",
            "11210000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import equinox as eqx\n",
        "\n",
        "def collect_activations_by_dataset(val_loader, model, state, layer_keys, skip_timesteps=0):\n",
        "    inference_model = eqx.nn.inference_mode(model)\n",
        "\n",
        "    activations_by_dataset = {}\n",
        "\n",
        "    testidxs = list(val_loader.sampler)  # or val_loader.sampler\n",
        "    print(\"batches announced:\", len(testidxs))\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Batches\")):\n",
        "        batch = {k: jax.device_put(np.array(v)) for k, v in batch.items()}\n",
        "        dataset_group_idxs = batch[\"dataset_group_idx\"]\n",
        "        inputs = batch[\"neural_input\"]\n",
        "        targets = batch[\"behavior_input\"]\n",
        "        mask = batch[\"mask\"][..., None]\n",
        "\n",
        "        state_out = state\n",
        "\n",
        "        for i in range(inputs.shape[0]):\n",
        "            pred, state_out, acts = inference_model.call_with_activations(\n",
        "                inputs[i], state_out, dataset_group_idxs[i], layer_keys\n",
        "            )\n",
        "\n",
        "            # Apply skip_timesteps\n",
        "            #acts0 = acts[0][skip_timesteps:]\n",
        "            #acts1 = acts[1][skip_timesteps:]\n",
        "            #targs = targets[i][skip_timesteps:]\n",
        "\n",
        "            ds_id = int(dataset_group_idxs[i])\n",
        "\n",
        "            if ds_id not in activations_by_dataset:\n",
        "                activations_by_dataset[ds_id] = {k: [] for k in acts.keys()}\n",
        "                activations_by_dataset[ds_id][\"targets\"] = []\n",
        "                activations_by_dataset[ds_id][\"neural_input\"] = []\n",
        "                activations_by_dataset[ds_id][\"mask\"] = []\n",
        "\n",
        "            # slice acts and targets\n",
        "            acts_sliced = {k: v[skip_timesteps:, :] for k, v in acts.items()}\n",
        "            targets_sliced = targets[i][skip_timesteps:, :]\n",
        "            inputs_sliced = inputs[i][skip_timesteps:, :]\n",
        "            mask_sliced = mask[i][skip_timesteps:, :]\n",
        "\n",
        "            # append dynamically for all keys except \"targets\"\n",
        "            for k, v in acts_sliced.items():\n",
        "                activations_by_dataset[ds_id][k].append(v)\n",
        "\n",
        "            # handle targets separately\n",
        "            activations_by_dataset[ds_id][\"targets\"].append(targets_sliced)\n",
        "            activations_by_dataset[ds_id][\"neural_input\"].append(inputs_sliced)\n",
        "            activations_by_dataset[ds_id][\"mask\"].append(mask_sliced)\n",
        "\n",
        "    # Stack into arrays\n",
        "    for ds_id in activations_by_dataset:\n",
        "        activations_by_dataset[ds_id] = {\n",
        "            k: jnp.stack(v, axis=0) for k, v in activations_by_dataset[ds_id].items()\n",
        "        }\n",
        "\n",
        "    return activations_by_dataset\n"
      ],
      "metadata": {
        "id": "g96AWUDfGog1"
      },
      "id": "g96AWUDfGog1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_keys = [\"ssm_x\", \"ssm_y\", \"ssm_post_glu\"]\n",
        "activations_2block = collect_activations_by_dataset(\n",
        "    val_loader=loader,   # your DataLoader\n",
        "    model=foundational_model_2block,          # model with patched call_with_activations\n",
        "    state=foundational_state_2block,              # initial recurrent state\n",
        "    layer_keys=layer_keys,\n",
        "    skip_timesteps=56         # same skip as before, or 0 for none\n",
        ")\n",
        "\n",
        "activations_4block = collect_activations_by_dataset(\n",
        "    val_loader=loader,   # your DataLoader\n",
        "    model=foundational_model_4block,          # model with patched call_with_activations\n",
        "    state=foundational_state_4block,              # initial recurrent state\n",
        "    layer_keys=layer_keys,\n",
        "    skip_timesteps=56         # same skip as before, or 0 for none\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PntFK1J6IJA6",
        "outputId": "d3db23f5-70d6-4a84-d8ec-1811e4cd5192"
      },
      "id": "PntFK1J6IJA6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batches announced: 357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batches: 100%|██████████| 1/1 [01:15<00:00, 75.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batches announced: 357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batches: 100%|██████████| 1/1 [01:43<00:00, 103.76s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from foundational_ssm.constants import DATASET_IDX_TO_GROUP_SHORT\n",
        "\n",
        "activations_group_2block = {str(k): v for k, v in activations_2block.items()}\n",
        "np.savez(\"/content/drive/MyDrive/Thesis/data/activations_reaching_2block_20250910_1.npz\", **activations_group_2block)\n",
        "activations_group_4block = {str(k): v for k, v in activations_4block.items()}\n",
        "np.savez(\"/content/drive/MyDrive/Thesis/data/activations_reaching_4block_20250910_1.npz\", **activations_group_4block)\n"
      ],
      "metadata": {
        "id": "ERWcVPOMWCRF"
      },
      "id": "ERWcVPOMWCRF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activations_group_2block = {DATASET_IDX_TO_GROUP_SHORT[k]: v for k, v in activations_2block.items()}\n",
        "activations_group_4block = {DATASET_IDX_TO_GROUP_SHORT[k]: v for k, v in activations_4block.items()}"
      ],
      "metadata": {
        "id": "ZCUsQCUi5zgx"
      },
      "id": "ZCUsQCUi5zgx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(activations_2block[0]['ssm_y_1'].shape)\n",
        "print(activations_2block[0]['neural_input'].shape)\n",
        "print(activations_2block[0]['targets'].shape)"
      ],
      "metadata": {
        "id": "K8rUjFzWlvja"
      },
      "id": "K8rUjFzWlvja",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[input_by_dataset[k]['neural_input_raw'].shape for k in activations_group_2block.keys()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuX3cJyj5qE_",
        "outputId": "318d53fe-0fa8-4c03-82eb-70471adb2dc5"
      },
      "id": "GuX3cJyj5qE_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(78, 944, 625),\n",
              " (19, 944, 625),\n",
              " (20, 944, 625),\n",
              " (12, 944, 625),\n",
              " (17, 944, 625),\n",
              " (47, 944, 625),\n",
              " (40, 944, 625),\n",
              " (124, 944, 625)]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.count_nonzero(activations_group_2block['pm_c_co']['neural_input'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lakJNeD6A5D",
        "outputId": "ee3a703f-c242-49f8-ffc4-3975847d036b"
      },
      "id": "3lakJNeD6A5D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "355518"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "it = iter(loader)\n",
        "for b in range(1000000):\n",
        "    try:\n",
        "        batch = next(it)\n",
        "    except StopIteration:\n",
        "        print(\"actually yielded:\", b)  # probably 23\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(\"error at batch\", b, \"->\", repr(e))\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "UB6IYZl1rPKf",
        "outputId": "91759a43-5dac-4020-c724-f1fde2842a33"
      },
      "id": "UB6IYZl1rPKf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3284697222.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2357034",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "f2357034",
        "outputId": "53e33008-399c-45c4-edf0-14bdaf5909e0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SequentialFixedWindowSampler.__init__() got an unexpected keyword argument 'prepend_history'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2695475457.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m get_brainset_train_val_loaders(\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mdataset_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mloader_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/foundational_ssm/loaders.py\u001b[0m in \u001b[0;36mget_brainset_train_val_loaders\u001b[0;34m(dataset_args, train_loader_cfg, val_loader_cfg, prepend_history, data_root)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m ):\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_max_neural_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_brainset_data_loader\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrain_loader_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_max_neural_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_brainset_data_loader\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mval_loader_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_trial'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mmax_neural_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_max_neural_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_max_neural_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/foundational_ssm/loaders.py\u001b[0m in \u001b[0;36mget_brainset_data_loader\u001b[0;34m(dataset_args, dataloader_args, sampler, split, sampler_args, sampling_rate, prepend_history, data_root)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msampling_intervals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sampling_intervals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0msampler_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     sampler = sampler_cls(\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0msampling_intervals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_intervals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler_args\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SequentialFixedWindowSampler.__init__() got an unexpected keyword argument 'prepend_history'"
          ]
        }
      ],
      "source": [
        "import multiprocessing as mp\n",
        "\n",
        "# Foundational SSM core imports\n",
        "from foundational_ssm.loaders import get_brainset_data_loader, get_brainset_train_val_loaders\n",
        "#from foundational_ssm.constants import DATA_ROOT\n",
        "from foundational_ssm.samplers import TrialSampler\n",
        "import os\n",
        "\n",
        "mp.set_start_method(\"spawn\", force=True) # otherwise causes deadlock on jax.\n",
        "\n",
        "data_root = '/content/drive/MyDrive/Thesis/data/' + DATA_ROOT # change to the folder holding the brainsets\n",
        "#DATA_ROOT = '/content/drive/MyDrive/Thesis/data/foundational_ssm/processed'\n",
        "config_dir = '/content/drive/MyDrive/Thesis/data/' # change\n",
        "dataset_args = {\n",
        "    'keep_files_open': False,\n",
        "    'lazy': True,\n",
        "    'config' : os.path.join(config_dir, 'reaching_analysis.yaml'),\n",
        "    #'split': 'val' # or 'train'\n",
        "}\n",
        "dataloader_args = {\n",
        "    'batch_size': 128, # Adjust per your system capacity\n",
        "    'num_workers': 4,\n",
        "    'persistent_workers': False\n",
        "}\n",
        "\n",
        "\n",
        "sampler = 'SequentialFixedWindowSampler'\n",
        "sampler_args = {\n",
        "                'window_length': 3.279,\n",
        "                'drop_short': True\n",
        "                }\n",
        "\n",
        "loader_cfg = {'sampler': sampler, 'sampler_args': sampler_args, 'dataloader_args': dataloader_args}\n",
        "\n",
        "#dataset, data_loader = get_brainset_data_loader(\n",
        "#    dataset_args=dataset_args,\n",
        "#    sampler = sampler,\n",
        "#    sampler_args = sampler_args,\n",
        "#    dataloader_args = dataloader_args,\n",
        "#    sampling_rate = 200,\n",
        "    #dataset_cfg = os.path.join(config_dir, 'reaching_analysis.yaml'),\n",
        "#    data_root = data_root,\n",
        "#    split = 'val'\n",
        "#)\n",
        "\n",
        "#{dataset_cfg = os.path.join(config_dir, 'reaching_new.yml'),\n",
        "\n",
        "\n",
        "# }\n",
        "\n",
        "\n",
        "get_brainset_train_val_loaders(\n",
        "    dataset_args,\n",
        "    loader_cfg,\n",
        "    loader_cfg,\n",
        "    prepend_history=0,\n",
        "    data_root=DATA_ROOT,\n",
        "    )\n",
        "\n",
        "sessions = dataset.get_session_ids() # list of sessions in your dataset\n",
        "sampling_intervals = dataset.get_sampling_intervals() # list of sampling intervals for each session"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kGnYwe8SGm96"
      },
      "id": "kGnYwe8SGm96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a814feb9",
      "metadata": {
        "id": "a814feb9"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db01d690",
      "metadata": {
        "id": "db01d690",
        "outputId": "bae3ddea-4c3e-4789-83eb-0b852d38a4bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Skipping 378.58696666666947 seconds of data due to short intervals. Remaining: 2508.435 seconds.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'val/r2_pm_c_co': 0.9161773920059204,\n",
              " 'val/r2_pm_c_rt': 0.8242394924163818,\n",
              " 'val/r2_pm_m_rt': 0.8003662824630737,\n",
              " 'val/r2_pm_m_co': 0.8416915535926819,\n",
              " 'val/r2_os_i_rt': 0.7526258230209351,\n",
              " 'val/r2_os_l_rt': 0.5253342390060425,\n",
              " 'val/r2_cs_j_co': 0.90641850233078,\n",
              " 'val/r2_cs_n_co': 0.9437413215637207,\n",
              " 'val/r2_avg': 0.813824325799942,\n",
              " 'val/r2_all': 0.9150385856628418,\n",
              " 'val/time': 26.957586765289307}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from foundational_ssm.utils.pretrain_utils import validate_one_epoch\n",
        "\n",
        "metrics = validate_one_epoch(\n",
        "    data_loader, model, state, skip_timesteps=56 # only when computing R2, we would keep this for analysis\n",
        ")\n",
        "metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0625cdf",
      "metadata": {
        "id": "e0625cdf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}